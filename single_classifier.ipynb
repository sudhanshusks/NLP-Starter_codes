{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For our text classification, we have to find some way to \"describe\" bits of data, which are labeled as either positive or negative for machine learning training purposes. \n",
    "\n",
    "#### These descriptions are called \"features\" in machine learning. For our project, we're just going to simply classify each word within a positive or negative review as a \"feature\" of that review. \n",
    "\n",
    "#### Then, as we go on, we can train a classifier by showing it all of the features of positive and negative reviews (all the words), and let it try to figure out the more meaningful differences between a positive review and a negative review, by simply looking for common negative review words and common positive review words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Algorithm\n",
    "#### The algorithm of choice, at least at a basic level, for text analysis is often the Naive Bayes classifier. Part of the reason for this is that text data is almost always massive in size. The Naive Bayes algorithm is so simple that it can be used at scale very easily with minimal process requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "Naive bayes Accuracy is :  81.0\n",
      "Most Informative Features\n",
      "             outstanding = True              pos : neg    =     13.5 : 1.0\n",
      "                   jolie = True              neg : pos    =      8.4 : 1.0\n",
      "                   mulan = True              pos : neg    =      8.2 : 1.0\n",
      "                  finest = True              pos : neg    =      7.6 : 1.0\n",
      "              schumacher = True              neg : pos    =      7.5 : 1.0\n",
      "                  seagal = True              neg : pos    =      7.5 : 1.0\n",
      "             wonderfully = True              pos : neg    =      7.4 : 1.0\n",
      "                 idiotic = True              neg : pos    =      7.1 : 1.0\n",
      "                   damon = True              pos : neg    =      6.0 : 1.0\n",
      "                   inept = True              neg : pos    =      6.0 : 1.0\n",
      "                   flynt = True              pos : neg    =      5.6 : 1.0\n",
      "                    lame = True              neg : pos    =      5.4 : 1.0\n",
      "                  wasted = True              neg : pos    =      5.4 : 1.0\n",
      "                   awful = True              neg : pos    =      5.3 : 1.0\n",
      "             beautifully = True              pos : neg    =      5.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "documents = []\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        documents.append((list(movie_reviews.words(fileid)), category))\n",
    "\n",
    "# first 1000 fileids in corpora are positive sentiment and next 1000 are negative, so random shuffle is used\n",
    "random.shuffle(documents)    \n",
    "#print(documents[2])        \n",
    "\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "    \n",
    "all_words = nltk.FreqDist(all_words)  # converting list of words to frequency distribution dictionary\n",
    "#print(all_words.most_common(15))\n",
    "#print(all_words[\"stupid\"])\n",
    "\n",
    "# taking only top 3000 words as features\n",
    "word_features = [w[0] for w in all_words.most_common(3000)]\n",
    "#word_features = list(all_words.keys())[:3000]\n",
    "\n",
    "# function to find words which are present in a document\n",
    "def find_features(document):\n",
    "    words= set(document)   # this is all unique words in the document\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)  # boolean of whether w is present in the document or not\n",
    "        \n",
    "    return features \n",
    "\n",
    "#print(find_features(movie_reviews.words('neg/cv000_29416.txt')))\n",
    "# featureset is a list of tuples of category and corresponding words boolean in documents\n",
    "featureset = [(find_features(rev), category) for (rev, category) in documents]   # rev is review words\n",
    "#featureset[:2]\n",
    "print(len(featureset))\n",
    "training_set = featureset[:1900]\n",
    "testing_set = featureset[1900:]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Naive bayes Accuracy is : \", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the classifier using pickle and loading it up again to predict on new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "Naive bayes Accuracy is :  87.0\n",
      "Most Informative Features\n",
      "             outstanding = True              pos : neg    =     11.2 : 1.0\n",
      "                   mulan = True              pos : neg    =      8.4 : 1.0\n",
      "                   damon = True              pos : neg    =      7.9 : 1.0\n",
      "                  seagal = True              neg : pos    =      7.8 : 1.0\n",
      "                  finest = True              pos : neg    =      7.6 : 1.0\n",
      "                 idiotic = True              neg : pos    =      7.4 : 1.0\n",
      "             wonderfully = True              pos : neg    =      7.3 : 1.0\n",
      "              schumacher = True              neg : pos    =      7.0 : 1.0\n",
      "                   inept = True              neg : pos    =      6.1 : 1.0\n",
      "                   jolie = True              neg : pos    =      5.8 : 1.0\n",
      "                 flubber = True              neg : pos    =      5.6 : 1.0\n",
      "                    lame = True              neg : pos    =      5.6 : 1.0\n",
      "                   waste = True              neg : pos    =      5.4 : 1.0\n",
      "              ridiculous = True              neg : pos    =      5.4 : 1.0\n",
      "                  wasted = True              neg : pos    =      5.3 : 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# save_classifier is the file opened in write byte mode and file name is naivebayes.pickle\\nsave_classifier = open(\"naivebayes.pickle\", \"wb\")\\npickle.dump(classifier, save_classifier)   # .dump() will save classifier in opened file\\nsave_classifier.close()\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "import pickle\n",
    "\n",
    "documents = []\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        documents.append((list(movie_reviews.words(fileid)), category))\n",
    "\n",
    "# first 1000 fileids in corpora are positive sentiment and next 1000 are negative, so random shuffle is used\n",
    "random.shuffle(documents)    \n",
    "#print(documents[2])        \n",
    "\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "    \n",
    "all_words = nltk.FreqDist(all_words)  # converting list of words to frequency distribution dictionary\n",
    "#print(all_words.most_common(15))\n",
    "#print(all_words[\"stupid\"])\n",
    "\n",
    "# taking only top 3000 words as features\n",
    "word_features = [w[0] for w in all_words.most_common(3000)]\n",
    "#word_features = list(all_words.keys())[:3000]\n",
    "# function to find words which are present in a document\n",
    "def find_features(document):\n",
    "    words= set(document)   # this is all unique words in the document\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)  # boolean of whether w in present in the document\n",
    "        \n",
    "    return features \n",
    "\n",
    "#print(find_features(movie_reviews.words('neg/cv000_29416.txt')))\n",
    "featureset = [(find_features(rev), category) for (rev, category) in documents]\n",
    "#featureset[:2]\n",
    "print(len(featureset))\n",
    "training_set = featureset[:1900]\n",
    "testing_set = featureset[1900:]\n",
    "\n",
    "#classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "classifier_f= open(\"naivebayes.pickle\", 'rb')\n",
    "classifier= pickle.load(classifier_f)\n",
    "classifier_f.close()\n",
    "\n",
    "\n",
    "print(\"Naive bayes Accuracy is : \", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "#uncomment below for saving the classifier\n",
    "'''\n",
    "# save_classifier is the file opened in write byte mode and file name is naivebayes.pickle\n",
    "save_classifier = open(\"naivebayes.pickle\", \"wb\")\n",
    "pickle.dump(classifier, save_classifier)   # .dump() will save classifier in opened file\n",
    "save_classifier.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
